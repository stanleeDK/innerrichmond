brew services start postgresql
brew services stopke postgresql
https://www.trulia.com/real_estate/Inner_Richmond-San_Francisco/1423/market-trends/

pull data from production 
heroku pg:pull DATABASE_URL innerrichmond_development --app innerrichmond
rails generate migration add_slug_to_neighborhoods slug:string

Deploy to production
git push heroku master
heroku run rake db:migrate
heroku pg:push innerrichmond_development DATABASE_URL --app innerrichmond

Geo Coding Key: AIzaSyCnmgExygLuo6BD7yq7-Rfu40BXAo2oUd4

Update on Associations 
1. For a one to many relationship, you can do the following and essentially establish the relationships which can be accessed via objects in code 

2. Establish the relationship between the two in the models
class Realdaycareprovider < ActiveRecord::Base
	belongs_to :neighborhood
end
class Neighborhood < ActiveRecord::Base
	has_many :realdaycareproviders
end

3. Also ensure the keys in the db are set up correctly. I.e. in the realdaycareproviders table have a neighborhood_id and in the neighborhoods table have a primary key that maps to the neighborhood_id column

4. Use rails! Access the objects in both directions 
n = Neighborhood.first
puts n.realdaycareproviders.first.name

daycares = Realdaycareprovider.find(3)
puts daycares.neighborhood.neighborhood_name


SCRAPING APPROACH
1. For each source create a scraper that writes to a csv file, trigger via rake
2. For each source, create a rake function to move the csv data into the relevant source's data table (one trulia table, on realtor.com table etc)
3. By giving each scrape/ingest-from-csv a number, we can then use a rake function to 
	3a. compare the new sales data to the repository of official addresses
	3b. if there's no official address, then add it 
	3c. add the sale history to the history table 

At step 2. we need to normalize addresses into the official addresses format so the matching becomes easy down stream. This means
1. capitalizing 
2. breaking things out into street_name, address_number, street_type 

Apr 23 
- creaetd a rake task to ingest raw csvs. This instance is for trulia, but can be extended. 
- use rake tasks to kick off scraping from now on. The scraping scripts will populate intermediary ingestion tables and a future process will map it to an actual properties table after a match has been attempted with the official addresses table 

May 2nd 
- hook up trulia scraper to rake and start getting new data for the trulia ingest table
- every time you scrape, I think you should create a new csv for each scrape that way you can check for errors before publishing to production. Put the csv into the ingested_trulia_properties table 



May 3rd 


July 1st
- added support for bootstrap by 
	- gem filing gem 'bootstrap-sass', '3.3.7' and then bundle install commanding 
	- change /assets/stylesheet/application.css to /application.sccs. Added @import "bootstrap-sprockets"; @import "bootstrap"; to invoke bootstrap in all 		views 
	- added google font support 
	- added custom css to the application.sccs file 

- Added pgsearch (gem and reference in official_addresses.rb) for autocomplete  (https://www.sitepoint.com/search-autocomplete-rails-apps/)

July 2nd 
NEXT STEPS 
- In the rake task match trulia recent sales to the official list of addresses, if no match then 
	- add the new address to the official tables db 
	- create an innerrichmond.com sales history table and record the recent sale from trulia there
- create core property model and child listing and recent_sales models. 
- get a postgres ui working to validate data (just use \x to see the data in the terminal)
- built an autocomplete with psearch (a postgres search capability) and it's slow as hell

	--------------------
Migrations to create
	1. Official addresses table - add a column to show where a new address has been added from 
	2. In the source ingest tables, add a column to represent the scrape id. 
https://blog.revillweb.com/bootstrap-tutorial-a-responsive-design-tutorial-with-twitter-bootstrap-3-cb6445c1e586

-----------------------------------------------------------------------------
Data Models
	Sales History
	1. id 
	2. official_address_id
	3. value 
	4. date
	5. size 
	6. source (trulia/realtor/ other?)
	7. source_id (foreign key into the raw ingest table)



July 15th 
- Major pivot to daycare 
- deployed to heroku
- https://devcenter.heroku.com/articles/getting-started-with-ruby#use-a-database


Nov 15th 
- Start scraping https://secure.dss.ca.gov for 
- Get the raw data into a raw data table 

1. Rails ID
2. Type: 


Nov 22 
- Build something to prevent duplicates then rescrape and show them the citations and violations 

dev 21 
- gotta do better with the de-duping. right now you only go through ccsf which may have multiple licenses per daycare. then you look for dupes using only the first license, thus leaving the additional license unchecked for whether that too exists in the dss group
